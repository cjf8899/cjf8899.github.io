---
title:  "VGG paper review"
last_modified_at: 2020-05-23 08:26:28-04:00
categories: 
  - Image Recognition
tags:
  - update
toc: true
toc_label: "Getting Started"
---

# Very Deep Convolutional Networks for Large-Scale Image Recognition
> Karen Simonyan, Andrew Zisserman. "Very Deep Convolutional Networks for Large-Scale Image Recognition." 2015.

## Abstract
conv 깊이에 증가에 따른 모델들을 평가하였다. Imagenet Challenge 2014에서 수상을 하였다.

## Introduction
이 논문에서는 conv의 깊이에 대해서 다룬다. 매우작은 필터를 사용하기 때문에 깊이를 꾸준히 증가 시킬 수 있다. 결과적으로 classification, localisation task에서 좋은 결과를 얻었다.

## Convnet configurations

![initial](https://user-images.githubusercontent.com/53032349/92396241-aa536280-f15f-11ea-98e2-1a14ee31c8b1.png)

공정한 환경에서 conv의 깊이를 증가하여 측정하였다. conv는  3*3필터, stride는 1, padding도 1, max-pooling은 stride 2, 2*2필터를 사용하였다. 3개의 FC중 처음 2개는 4096채널, 나머지 하나는 1000채널 을 사용하였다. 모든 층엔 ReLU를 사용하였다.






## Classification Framework
batch size는 256, momentum은 0.9, 처음 FC두개에 dropout 0.5사용, weight decay(L2 norm), Learning rate는 0.01을 사용하고 서서히 감소하시켰다. 224*224고정 크기를 얻기위해 임의로 crop하였다. 또한 random horizontal과 RGB의 색상 shift를 사용하였다.


## Classification Experiments

![initial](https://user-images.githubusercontent.com/53032349/92396381-e5ee2c80-f15f-11ea-961e-9217017dbf16.png)
![initial](https://user-images.githubusercontent.com/53032349/92396502-10d88080-f160-11ea-8d6c-8b7703724d66.png)

single test scale 에서 A와 A-LRN의 성능차이가 없어 B~E 모델은 깊이에 따른 실험을 하였다. 깊어질수록 향상하였다.
multi-crop과 dense evaluation 사용하여 multi test scale 를 하였다. multi test scale을 사용한 모델이  조금 더 성능이 높았다.

## Conclusion
깊이가 깊어질수록 성능은 좋아지는 것을 확인하였다. 하지만 FC가 3개이기 때문에 파라미터수가 증가하여 메모리 공간의 차지가 심하다는 단점이 있다.


















