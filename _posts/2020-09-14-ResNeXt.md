---
title:  "ResNeXt paper review"
last_modified_at: 2020-09-14 00:00:00 -0400
categories: 
  - Image Recognition
tags:
  - update
toc: true
toc_label: "Getting Started"
---

# Aggregated Residual Transformations for Deep Neural Networks
> Saining Xie, Kaiming He, et al. ["Aggregated Residual Transformations for Deep Neural Networks."](https://arxiv.org/abs/1611.05431) Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.

# Abstract
cardinality라는 새로운 기법을 제시하였다. <br>
cardinality를 쓰는 것이 네트워크가 더 깊어지거나 넓어지는 것보다 효과적이다.<br>
ResNet보다 더 좋은 성능을 갖고 있다.

# Introduction

![initial](https://user-images.githubusercontent.com/53032349/93086132-9ebcea00-f6d1-11ea-81a8-c8807a9c3276.png)

VGG와 같이 동일한 모양의 블록을 쌓는 구조를 갖는다.<br>
하이퍼파라미터의 선택이 많아지면 특정 데이터셋에 대한 over adapting이 발생할 수 있다.<br>
인셉션 모듈은 계산 복잡성이 매우 낮다.<br>
이 논문에서는 VGG/ResNet 전략을 사용하고 split-transform-merge 방법을 사용할 것이다.<br>
ResNet 성능을 능가, 정확도는 높이는 것은 상대적으로 쉽지만 복잡성을 유지하면서 정확도를 높이는 방법은 드물다<br>
ResNeXt는 ImageNet에서 ResNet-101 / 152, ResNet200, Inception-v3 및 Inception-ResNet-v2보다 성능이 뛰어나다.<br>
ResNet-200 보다 더 나은 정확도를 달성 할 수 있지만 복잡성은 50 %에 불과하다

# Related Work
* Multi-branch convolutional networks.
* Grouped convolutions. : 알렉스넷에서 그룹컨브를 사용하지만 성능향상을 위해 사용하지는 않음
* Compressing convolutional networks
* Ensembling. : Resnext는 앙상블이 아니다.

Method
![initial](https://user-images.githubusercontent.com/53032349/93086188-b3997d80-f6d1-11ea-99f0-4ee31aa948aa.png)

VGG / ResNet에서 영감을 얻은 두 가지 규칙
1. 동일한 사이즈의 spacial map을 사용한다면 블록은 하이퍼파라미터를 공유한다.spatial map이 2배 다운샘플링 될 때마다 폭이 2배 늘어난다.<br>
2. FLOP(계산적 복잡성)이 모든 블록에 대해 거의 동일하다.<br>
이 두 가지 규칙을 통해 design space를 줄이고 몇가지 핵심 요소에만 집중할 수 있다.<br>
계산적 복잡도는  multi-add이다.

split-transform-merge방법

![initial](https://user-images.githubusercontent.com/53032349/93086789-9c0ec480-f6d2-11ea-9bdb-1a08a0b5b4ad.png)

* Splitting : 그림에서 보듯이 x가 x1, x2, ... xd순서로 잘린다.
* Transforming : 저차원인  x1, x2, ...xd에 각각 가중치를 곱하여 크기를 변화시킨다.
* Aggregating : 변환된 저차원들을 합친다.

network-in-network가 아닌 network-in-neuron으로 깊이대신 새로운 차원으로 확장한다. <br>
bottleneck 모양을 구축 : 차원을 줄였다가 늘렸다 할 때 연산시간을 줄이기위해

![initial](https://user-images.githubusercontent.com/53032349/93086874-be084700-f6d2-11ea-9f43-dc05661d61ce.png)

y= output, x=input, C = cardinality, Ti = arbitrary function

![initial](https://user-images.githubusercontent.com/53032349/93086963-ded09c80-f6d2-11ea-8535-259f4bc946e0.png)

resnet의 FLOPs은 256*64 + 3*3*64*64 + 34*256 = 70 k정도<br>
resnext의 일반식은  C * (256* d + 3*3*d*d + d*256)이다.<br>
따라서 resnet과 resnext의 FLOPs을 비교하였을 때 C= 32일때는 d 는 4가 되어야 70k 정도이다. 이렇게 FLOPs을 유지함으로써 Cardinality의 영향을 집중하게한다.


# Implementation details
* dataset : imagenet
* input image size : 224*224
* augmentation : randomly cropped
* Short cut : ResNet의 방식
* batch size : 256 on 8 GPU
* weight decay : 0.0001
* momentum : 0.9
* learning late : 0.1
* Conv block + Batch Normlization + ReLU

# Experiments

cardinality = 32, bottle neck width = 4d를 ResNext-50(32x4d)로 간단하게 나타냄 <br>
## Cadinality vs Width

![initial](https://user-images.githubusercontent.com/53032349/93087148-20f9de00-f6d3-11ea-9dd2-1c4eba55ea6b.png)

**cardinality가 증가함에 따라 top-1 error가 낮아지고 있음, resnet과 비교해 보았을때도 성능이 좋음**

## Increasing Cardinality vs Deeper/Wider

![initial](https://user-images.githubusercontent.com/53032349/93087228-3cfd7f80-f6d3-11ea-9c94-2c88cfb358a2.png)

우선 resnet에서 깊은 resnet200과 넓은 resnet wider를 비교 : resnet wider가 더 좋음<br>
위 결과에 따라 넓이와 cardinality 비교<br>
ResNeXt101 (2x64d)와 ResNeXt101(64x4d) 비교 :<br>
**cardinality가 높은 것이 성능이 더 좋음**<br>

마지막 shortcut의 중요성을 보여주는 실험이다

![initial](https://user-images.githubusercontent.com/53032349/93087291-51417c80-f6d3-11ea-8397-d428a04dea15.png)

# Code Implement
ResNeXt code : <https://github.com/cjf8899/Pytorch_ResNeXt>



















